# Project SYNAPSE ðŸ§ 

**A Real-Time Embedded Vision System for Assistive Navigation and Contextual Awareness**

![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)
![License](https://img.shields.io/badge/License-MIT-green.svg)
![Status](https://img.shields.io/badge/Status-Prototype-orange.svg)

---

## ðŸ“‹ Overview

SYNAPSE is an intelligent, wearable assistive device designed for visually impaired individuals. It provides real-time environmental understanding through computer vision and AI, offering:

- **Proactive Navigation** - Trajectory-based warnings for approaching obstacles
- **Scene Understanding** - Natural language descriptions of surroundings
- **Text Recognition** - OCR for signs, labels, and documents
- **Social Awareness** - Face detection and emotion recognition (planned)
- **Interactive Control** - User-driven, on-demand information retrieval

---

## ðŸŽ¯ Key Features

### 1. Object Detection & Tracking
- YOLOv11-Nano for real-time object detection
- SORT-based tracking with stable IDs
- **Novel**: Trajectory prediction for proactive warnings

### 2. Optical Character Recognition
- EasyOCR for "text-in-the-wild" extraction
- Preprocessing pipeline for enhanced accuracy
- Text-to-speech output

### 3. Scene Captioning
- BLIP (Salesforce) for natural language scene descriptions
- Context-aware formatting
- <3 second latency on CPU

### 4. Interactive Voice Interface
- On-demand module activation
- **Novel**: Solves information overload problem
- Natural language output

---

## ðŸš€ Quick Start

### Prerequisites

- Python 3.9 or higher
- Webcam
- Windows/Linux/MacOS
- 8GB RAM minimum

### Installation

```
# Clone repository
git clone https://github.com/YOUR_USERNAME/synapse-prototype.git
cd synapse-prototype

# Create virtual environment
python -m venv synapse-env

# Activate virtual environment
# Windows:
.\synapse-env\Scripts\activate
# Linux/Mac:
source synapse-env/bin/activate

# Install dependencies
pip install -r requirements.txt
```

**Note**: First run will download AI models (~2GB total). Requires internet connection initially.

### Run the System

```
python main.py
```

---

## ðŸŽ® Controls

| Key | Action | Description |
|-----|--------|-------------|
| `W` | What's ahead? | Get summary of detected objects with audio |
| `D` | Describe scene | Generate natural language scene description |
| `R` | Read text | Extract and read text from current view |
| `T` | Toggle warnings | Enable/disable automatic trajectory warnings |
| `Q` | Quit | Exit system |

---

## ðŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  User Interface                     â”‚
â”‚              (Voice Commands + Audio)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Intelligence & Control Layer              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Command    â”‚      â”‚  Adaptive Power         â”‚  â”‚
â”‚  â”‚  Processor   â”‚      â”‚  Manager (planned)      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Processing Pipeline                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ YOLO +  â”‚  â”‚  BLIP   â”‚  â”‚ OCR  â”‚  â”‚   Face   â”‚   â”‚
â”‚  â”‚ Tracker â”‚  â”‚Caption. â”‚  â”‚      â”‚  â”‚ (planned)â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Hardware Layer                        â”‚
â”‚         Camera | Mic | IMU (optional)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“Š Performance Metrics

| Metric | Current (CPU) | Target (Pi + Optimization) |
|--------|--------------|----------------------------|
| Detection FPS | 9 FPS | 15-20 FPS |
| Tracking FPS | 4-5 FPS | 10-15 FPS |
| OCR Latency | 2-3s | <2s |
| Caption Latency | 3s | <2s |
| Total System Latency | <3s | <2s |
| Memory Usage | <3GB | <2GB |

---

## ðŸ”¬ Novel Contributions

### 1. Proactive Navigation
- First wearable system with trajectory-based collision prediction
- Shifts from reactive ("obstacle at 2m") to proactive ("person approaching from right")
- Demonstrated 17 directional events in testing

### 2. User-Centric Information Control
- Solves 7-year-old "information overload" problem (identified in 2018 HCI study)
- Voice-activated on-demand architecture
- User pulls information vs. system pushing continuously

### 3. Complete Multi-Modal Integration
- Only system combining detection, OCR, scene understanding, and social cues
- All processing on-device (privacy-first)
- Unified natural language interface

### 4. Context-Aware Power Management (Planned)
- IMU-based activity detection
- Dynamic model scheduling (stationary/walking/running modes)
- Target: 6+ hour battery life on Raspberry Pi 4

---

## ðŸ“‚ Project Structure

```
synapse-prototype/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ io/
â”‚   â”‚   â”œâ”€â”€ camera.py          # Threaded camera capture
â”‚   â”‚   â”œâ”€â”€ tts_output.py      # Text-to-speech engine
â”‚   â”‚   â””â”€â”€ audio_input.py     # Microphone input (planned)
â”‚   â”œâ”€â”€ vision/
â”‚   â”‚   â”œâ”€â”€ detector.py        # YOLOv11 wrapper
â”‚   â”‚   â”œâ”€â”€ tracker.py         # DeepSORT wrapper
â”‚   â”‚   â”œâ”€â”€ simple_tracker.py  # Lightweight tracker
â”‚   â”‚   â”œâ”€â”€ ocr.py             # EasyOCR wrapper
â”‚   â”‚   â”œâ”€â”€ captioner.py       # BLIP wrapper
â”‚   â”‚   â””â”€â”€ face_emotion.py    # Face recognition (planned)
â”‚   â”œâ”€â”€ logic/
â”‚   â”‚   â”œâ”€â”€ output_generator.py # Natural language generation
â”‚   â”‚   â””â”€â”€ command_processor.py # Command parsing (planned)
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ fps_counter.py     # Performance monitoring
â”‚       â””â”€â”€ logger.py          # Event logging (planned)
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_camera.py
â”‚   â”œâ”€â”€ test_detector.py
â”‚   â”œâ”€â”€ test_tracker.py
â”‚   â”œâ”€â”€ test_ocr.py
â”‚   â””â”€â”€ test_caption.py
â”œâ”€â”€ config.yaml                # Configuration file
â”œâ”€â”€ main.py                    # Main orchestrator
â”œâ”€â”€ requirements.txt           # Dependencies
â””â”€â”€ README.md                  # This file
```

---

## ðŸ§ª Testing

Individual module tests:

```
# Test camera input
python test_camera.py

# Test object detection
python test_detector.py

# Test tracking
python test_tracker.py

# Test OCR
python test_ocr.py

# Test scene captioning
python test_caption.py
```

---

## ðŸ› ï¸ Technology Stack

| Component | Technology |
|-----------|-----------|
| Object Detection | YOLOv11-Nano (Ultralytics) |
| Tracking | SORT / DeepSORT |
| OCR | EasyOCR + Tesseract |
| Scene Captioning | BLIP (Salesforce) |
| Face Recognition | DeepFace + MediaPipe (planned) |
| TTS | pyttsx3 (offline) |
| Wake Word | Porcupine (planned) |
| Vision Processing | OpenCV |
| Deep Learning | PyTorch, Transformers |

---

## ðŸ“ˆ Roadmap

### Phase 1: Core Prototype âœ… (Current)
- [x] Camera input with threading
- [x] Object detection (YOLO)
- [x] Basic tracking with direction prediction
- [x] OCR with TTS
- [x] Scene captioning
- [x] Unified interface

### Phase 2: Optimization (In Progress)
- [ ] Model quantization (TFLite INT8)
- [ ] Raspberry Pi 4 deployment
- [ ] Improved tracking stability
- [ ] IMU integration for power management
- [ ] Wake-word activation

### Phase 3: Advanced Features
- [ ] Face recognition + emotion detection
- [ ] Depth estimation (monocular)
- [ ] Outdoor navigation mode
- [ ] Multi-language support

### Phase 4: Production
- [ ] User studies with visually impaired participants
- [ ] Wearable hardware enclosure design
- [ ] Battery optimization (6+ hours)
- [ ] Publication preparation

---

## ðŸ“š Literature & References

Based on comprehensive survey of 20 papers (2007-2025):

**Key Papers:**
- YOLO-LITE (2018) - Embedded object detection
- SORT (2016) - Real-time tracking
- BLIP (2022) - Vision-language pre-training
- Information Overload Study (2018) - User-centric design

**Gaps Addressed:**
1. Single-modality focus â†’ Multi-modal integration
2. Reactive navigation â†’ Proactive trajectory prediction
3. Information overload â†’ User-controlled interface
4. Cloud dependency â†’ Complete on-device processing

See `docs/literature_survey.md` for full analysis.

---

## ðŸ‘¥ Team

- **Team Size**: 4 members
- **Institution**: [M S Ramaiah Institute of Technology]
- **Course**: Final Year Engineering Project
- **Supervisor**: [Dr. Sini Alex]

---

## ðŸ“„ License

MIT License - See LICENSE file for details

---

## ðŸ™ Acknowledgments

- Ultralytics for YOLOv11
- Salesforce for BLIP model
- JaidedAI for EasyOCR
- HuggingFace for Transformers library
- Open-source community

---

## ðŸ“§ Contact

For questions or collaboration:
- **Email**: [shreyaspatil171@gmail.com]
- **Project Lead**: [Shreyas Patil]

---

## ðŸ“¸ Demo

**Video**: [Link to demo video]

**Screenshots**:
- Live tracking with directional warnings
- Scene captioning output
- OCR text extraction

---

**Built with â¤ï¸ for accessibility and inclusion**

*"Technology should empower everyone, regardless of ability."*

---

## ðŸ› Known Issues

- Tracking ID stability needs improvement (optimization in Phase 2)
- OCR accuracy varies with lighting (70-85% currently)
- CPU performance bottleneck (will improve with Pi + quantization)

See [Issues](https://github.com/shreyas1017/synapse-prototype/issues) for tracking.